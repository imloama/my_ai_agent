# LLM

随着chatgpt的兴起，LLM也发展的越来越快，开源项目如llama3/mistral/通义千问等等，都是能力不错的大模型。随着大家越来越关注推理效率，像llama.cpp等项目的不断迭代，目前很多开源项目已经支持类似于llama.cpp的方案。

为了对接LLM，常用模型如,langchain、llama_index等，提供了非常丰富的接口和示例。

对于llm模型的运行，可以直接运行模型推理代码，或者通过llama.cpp等项目。本次推荐基于ollama模型来运行模型。

- [ollama](https://github.com/ollama/ollama)
- [llama_index](https://github.com/run-llama/llama_index)

## ollama

  官方网站：[https://ollama.com/](https://ollama.com/)
  
  开源地址：[https://github.com/ollama/ollama](https://github.com/ollama/ollama)


  ollama是模型运行工具，并提供api，方便使用，支持多种开源模型，且本身支持跨平台使用。ollama下载模型时，会下载到用户目录的.ollama下，因为模型文件一般都比较大，所以设置环境变量，将下载的模型保存到新的目录中，环境变量名为：`OLLAMA_MODELS`。
  ollama即支持远程下载模型，也支持运行本地模型。
  基于ollama的webui也非常丰富，后续扩展也是非常方便。


### 常用命令

- pull: 下载模型文件，可以在[https://ollama.com/library](https://ollama.com/library)查询可用的模型名称，如：`ollama pull llama3`
- rm: 删除模型，如：`ollama rm llama3`
- show: 显示模型信息，如：`ollama show llama3`
- list: 显示本地下载的所有模型，如：`ollama list`
- run: 运行模型，如：`ollama run llama3`


### 示例

  安装ollama后，项目中安装官方的python API库[ollama-python](https://github.com/ollama/ollama-python)来使用。


### 延深思考

- 模型不同的量化版本差异，float16/int8/int4
- 怎么微调大模型





## RAG



而对于向量数据库，推荐基于postgres的pgvector插件来实现。